{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '../Source_Code')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import importlib\n",
    "import pandas as pd\n",
    "import seaborn\n",
    "from IPython.display import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import Transformer as tnsf\n",
    "import preprocess as ad\n",
    "\n",
    "import copy\n",
    "from matplotlib.colors import LogNorm\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "importlib.reload(ad)\n",
    "importlib.reload(tnsf)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = \"cuda\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_generate(name, window_size=10):\n",
    "    hdfs = set()\n",
    "    # hdfs = []\n",
    "    with open( name, 'r') as f:\n",
    "        for ln in f.readlines():\n",
    "            ln = list(map(lambda n: n, map(int, ln.strip().split())))\n",
    "#             ln = ln + [0] * (window_size + 1 - len(ln))\n",
    "            hdfs.add(tuple(ln))\n",
    "            # hdfs.append(tuple(ln))\n",
    "    print('Number of sessions({}): {}'.format(name, len(hdfs)))\n",
    "    return hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'preprocess' from '../Source_Code/preprocess.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(ad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mat(dataloader, model, layers, heads, file, enc =True, dec = False):\n",
    "    \n",
    "    enc_att = np.zeros((449,449))\n",
    "    enc_tp_att = np.zeros((449,449))\n",
    "    enc_fn_att = np.zeros((449,449))\n",
    "    \n",
    "    if dec:\n",
    "        dec_att = np.zeros((449,449))\n",
    "        dec_tp_att = np.zeros((449,449))\n",
    "        dec_fn_att = np.zeros((449,449))\n",
    "    \n",
    "    src_mask = Variable(torch.ones(1, 1, window_size + 1)).to(device)\n",
    "    bos = torch.ones((1, ),dtype = int).to(device)\n",
    "    \n",
    "    print(\"********** ********** ********** ********** **********\")\n",
    "    print(\"      ********** FIRST INPUT SEQUENCE ********** \")\n",
    "    print(\"********** ********** ********** ********** **********\")\n",
    "    print(\"\\n\")\n",
    "    sequence_cnt = 10\n",
    "    anomaly_timeline = []\n",
    "    line_number = 1\n",
    "    for line in dataloader:\n",
    "        if line_number%25==0:\n",
    "            print(\"Current Sequence Number:\",line_number)\n",
    "        else:\n",
    "            pass\n",
    "        line_number += 1\n",
    "        for i in range(len(line) - window_size):\n",
    "            seq = line[i:i + window_size]\n",
    "            \n",
    "            #for predicting 10\n",
    "            label = line[i+window_size:(i+window_size)+window_size]\n",
    "            \n",
    "            #for predicting only 1\n",
    "            ###label = line[i+window_size:(i+window_size)+1]\n",
    "            ###print(\"Sequence: {}\".format(seq))\n",
    "            ###ad.backtrace(seq, \"linux.log\", \"Spell\")\n",
    "            \n",
    "\n",
    "            t1 = torch.cat((bos, torch.tensor(seq, dtype = torch.int).to(device))).unsqueeze(0)\n",
    "            t2 = torch.tensor(label, dtype = torch.int).to(device).unsqueeze(0)\n",
    "\n",
    "            src = Variable(t1, requires_grad =False)\n",
    "            tgt = Variable(t2, requires_grad =False)\n",
    "\n",
    "            print(\"---------- ---------- ---------- ---------- ----------\")\n",
    "            print(\"Input Sequence: {}\".format(seq))\n",
    "            print(\"Label: {}\".format(label))\n",
    "            print(\"\\n\")\n",
    "\n",
    "#             print(\"~~~~~~~~~~ ATTENTION WEIGHTS ON GREEDY DECODE ~~~~~~~~~~\")\n",
    "#             print(\"\\n\")\n",
    "            ###tgt_pred = tnsf.predict(model, src, src_mask, tgt, max_len = len(tgt)+1, start_symbol = 1, g = 10)\n",
    "            ###tgt_pred = tnsf.greedy_decode(model, src, src_mask, tgt, len(label)+1, 1, True, g=10, halt = True, layers = layers, heads = heads)\n",
    "            tgt_pred, candidates = tnsf.greedy_decode(model, src, src_mask, tgt, 2, 1, True, g=10, halt = True, layers = layers, heads = heads)\n",
    "\n",
    "            # List for every deteected anomaly\n",
    "            if tgt_pred.cpu().detach().numpy() in candidates.cpu().detach().numpy():\n",
    "                anomaly_timeline.append(0)\n",
    "            else:\n",
    "                anomaly_timeline.append(1)\n",
    "                \n",
    "            ###print(\"---------- ---------- ---------- ---------- ----------\")\n",
    "            ###print(\"Input Sequence: {}\".format(seq))\n",
    "            ###print(\"Label: {}\".format(label))\n",
    "            ###print(\"Pred: {}\".format(tgt_pred[0][0]))\n",
    "\n",
    "            print(\"\\n\")\n",
    "            print(\"~~~~~~~~~~ SEQUENCE BACKTRACE ~~~~~~~~~~\")\n",
    "            ad.backtrace(seq, \"linux.log\", \"Spell\")\n",
    "            print(\"\\n\")\n",
    "            print(\"~~~~~~~~~~ LABEL BACKTRACE ~~~~~~~~~~\")\n",
    "            ad.backtrace(label[:1], \"linux.log\", \"Spell\") \n",
    "            print(\"\\n\")\n",
    "            print(\"~~~~~~~~~~ PREDICTION BACKTRACE [Top 4] ~~~~~~~~~~\")\n",
    "            ###ad.backtrace(tuple(tgt_pred.cpu().detach().numpy()[0]) , \"linux.log\", \"Spell\")\n",
    "            ad.backtrace(tuple(np.flip(candidates.cpu().detach().numpy()[6:])) , \"linux.log\", \"Spell\")\n",
    "            print(\"\\n\")\n",
    "            \n",
    "            enc_weights = 0\n",
    "            \n",
    "            for layer in range(layers):\n",
    "                for h in range(heads):\n",
    "                    ###print(\"Layer:{} , Head: {}\".format(layer,h))\n",
    "                    enc_weights += model.encoder.layers[layer].self_attn.attn[0, h].data[:len(label), 1:len(seq)+1][0]\n",
    "                    ###print(\"~~~~~~~~~~ ENCODER ATTENTION WEIGHTS ~~~~~~~~~~\")\n",
    "                    ###print(enc_weights)\n",
    "                    ###print(\"\\n\")\n",
    "                    \n",
    "            if dec:\n",
    "                dec_weights = 0\n",
    "                for layer in range(layers):\n",
    "                    for h in range(heads):\n",
    "                        ###print(\"Decoder Layer:{} , Head: {}\".format(layer,h))                    \n",
    "                        dec_weights += model.decoder.layers[layer].self_attn.attn[0, h].data[:len(label), :len(seq)][0]\n",
    "                        ###if you are predicting 10 use the code below\n",
    "                        ###dec_weights += model.decoder.layers[layer].self_attn.attn[0, h].data[:len(label), :len(seq)]\n",
    "                        ###print(\"~~~~~~~~~~ DECODER ATTENTION WEIGHTS ~~~~~~~~~~\")\n",
    "                        ###print(dec_weights)\n",
    "                        ###print(\"\\n\")\n",
    "            \n",
    "            print(\"########## END OF INFO FOR LOG SEQUENCE {} ##########\".format(sequence_cnt))\n",
    "            print(\"\\n\")\n",
    "            print(\"\\n\")\n",
    "            print(\"\\n\")\n",
    "\n",
    "            sequence_cnt += 1\n",
    "            \n",
    "            print(\"********** ********** ********** ********** **********\")\n",
    "            print(\"     ********** INPUT SEQUENCE {} - {} **********\".format(sequence_cnt-9,sequence_cnt))\n",
    "            print(\"********** ********** ********** ********** **********\")\n",
    "            print(\"\\n\")\n",
    "            \n",
    "            ###print(label[0],tgt_pred.cpu().detach().numpy())\n",
    "            ###print(label[0] in tgt_pred.cpu().detach().numpy())\n",
    "            ###if label[0] not in tgt_pred.cpu().detach().numpy():\n",
    "            ###    print(label[0],tgt_pred)\n",
    "            ###else:\n",
    "            ###    pass\n",
    "            \n",
    "            for i in range(len(tgt_pred)):\n",
    "#             uncomment if you are using 10 predictions instead of 1\n",
    "                for j in range(len(tgt_pred)):\n",
    "                        enc_att[seq[i],label[j]] += enc_weights[i]  \n",
    "#                 enc_att[seq[i], label[0]] += enc_weights[i]\n",
    "\n",
    "            \n",
    "            if -1 in tgt_pred:\n",
    "                for i in range(len(tgt_pred)):\n",
    "                #uncomment if you are using 10 predictions instead of 1\n",
    "                    for j in range(len(tgt_pred)):\n",
    "                        enc_tp_att[seq[i],label[j]] += enc_weights[i]\n",
    "#                     enc_tp_att[seq[i],label[0]] += enc_weights[i]\n",
    "            else:\n",
    "                for i in range(len(tgt_pred)):\n",
    "                #uncomment if you are using 10 predictions instead of 1\n",
    "                    for j in range(len(tgt_pred)):\n",
    "                        enc_fn_att[seq[i],label[j]] += enc_weights[i]    \n",
    "#                     enc_fn_att[seq[i], label[0]] += enc_weights[i]\n",
    "            \n",
    "            if dec:\n",
    "                for i in range(len(tgt_pred)):\n",
    "                    for j in range(len(tgt_pred)):\n",
    "                        dec_att[seq[i], label[j]] += dec_weights[i]\n",
    "\n",
    "                if -1 in tgt_pred:\n",
    "#                    print(\"halt\")\n",
    "                    for i in range(len(tgt_pred)):\n",
    "                        for j in range(len(tgt_pred)):\n",
    "                            dec_tp_att[seq[i],label[j]] += dec_weights[i]\n",
    "                else:\n",
    "                    for i in range(len(tgt_pred)):\n",
    "                        for j in range(len(tgt_pred)):\n",
    "                            dec_fn_att[seq[i], label[j]] += dec_weights[i]    \n",
    "            \n",
    "\n",
    "#     with open(\"Attention-Based_Weights/all_\" + file + '_attention.npy', 'wb') as f:\n",
    "#         np.save(f, enc_att)\n",
    "                        \n",
    "#     with open(\"Attention-Based_Weights/tp_\" + file + '_attention.npy', 'wb') as f:\n",
    "#         np.save(f, enc_tp_att)\n",
    "        \n",
    "#     with open(\"Attention-Based_Weights/fn_\" + file + '_attention.npy', 'wb') as f:\n",
    "#         np.save(f, enc_fn_att)        \n",
    "    \n",
    "    enc_att_normalized =(enc_att-np.min(enc_tp_att))/(np.max(enc_att)-np.min(enc_att))\n",
    "    enc_tp_att_normalized =(enc_tp_att-np.min(enc_tp_att))/(np.max(enc_tp_att)-np.min(enc_tp_att))\n",
    "    enc_fn_att_normalized =(enc_fn_att-np.min(enc_fn_att))/(np.max(enc_fn_att)-np.min(enc_fn_att))\n",
    "#     print(enc_fn_att_normalized[enc_fn_att_normalized>0])\n",
    "    if dec:\n",
    "        dec_att_normalized =(dec_att-np.min(dec_tp_att))/(np.max(dec_att)-np.min(dec_att))\n",
    "        dec_tp_att_normalized =(dec_tp_att-np.min(dec_tp_att))/(np.max(dec_tp_att)-np.min(dec_tp_att))\n",
    "        dec_fn_att_normalized =(dec_fn_att-np.min(dec_fn_att))/(np.max(dec_fn_att)-np.min(dec_fn_att))\n",
    "\n",
    "#     with open(\"Attention-Based_Weights/all_\" + file + '_normalized_attention.npy', 'wb') as f:\n",
    "#             np.save(f, enc_att_normalized)  \n",
    "#     with open(\"Attention-Based_Weights/tp_\" + file + '_normalized_attention.npy', 'wb') as f:\n",
    "#             np.save(f, enc_tp_att_normalized)  \n",
    "#     with open(\"Attention-Based_Weights/fn_\" + file + '_normalized_attention.npy', 'wb') as f:\n",
    "#             np.save(f, enc_fn_att_normalized)              \n",
    "\n",
    "    # Plot Anomaly Detection Timeline\n",
    "    fig = plt.figure(figsize = (15, 5))\n",
    "    x = range(11,len(anomaly_timeline)+11)\n",
    "    plt.xlabel(\"Log Timeline\", fontsize=14)\n",
    "    plt.ylabel(\"Anomaly = 1, Normal =0\", fontsize=14)\n",
    "    plt.title(\"Anomaly Detection Timelline\", fontsize=14)\n",
    "    plt.bar(x, anomaly_timeline, color = \"blue\")\n",
    "    plt.xlim([11, len(anomaly_timeline)+11])\n",
    "    #plt.setp(axs[1], yscale = 'log', xlabel=\"Log Key (Input Sequences)\")\n",
    "    plt.tight_layout()\n",
    "    # plt.savefig('GonnaCry_bar_all.png')\n",
    "    plt.show()        \n",
    "            \n",
    "#     return (enc_att_normalized, enc_tp_att_normalized, enc_fn_att_normalized), (dec_att_normalized, dec_tp_att_normalized, dec_fn_att_normalized)\n",
    "    return (enc_att_normalized, enc_tp_att_normalized, enc_fn_att_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sessions(../CTDD_Dataset/Sample_Cyber_Threat_Log_Keys/jy-ntp-ddos-amplification-atk-tool-t1498): 1\n"
     ]
    }
   ],
   "source": [
    "test_abnormal_loader = test_generate(\"../CTDD_Dataset/Sample_Cyber_Threat_Log_Keys/jy-ntp-ddos-amplification-atk-tool-t1498\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"../Saved_Models/global_models_practicum.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'Transformer' from '../Source_Code/Transformer.py'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model information\n",
    "layers = 1\n",
    "heads = 1\n",
    "WINDOW_SIZE = 10\n",
    "window_size = 10\n",
    "importlib.reload(ad)\n",
    "importlib.reload(tnsf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** ********** ********** ********** **********\n",
      "      ********** FIRST INPUT SEQUENCE ********** \n",
      "********** ********** ********** ********** **********\n",
      "\n",
      "\n",
      "---------- ---------- ---------- ---------- ----------\n",
      "Input Sequence: (418, 258, 259, 224, 260, 260, 260, 260, 260, 260)\n",
      "Label: (260, 260, 263, 264, 264, 264, 164, 265, 266, 267)\n",
      "\n",
      "\n",
      "^^^^^^^^^^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "Candidate logs:  tensor([ 20, 263, 164,  51,  32, 149, 260, 264,   8,   7], device='cuda:0')\n",
      "\n",
      "\n",
      "~~~~~~~~~~ ~~~~~~~~~~ ~~~~~~~~~~ ~~~~~~~~~~ ~~~~~~~~~~\n",
      "Probabilities:  tf.Tensor([0.0169 0.0212 0.0230 0.0320 0.0370 0.0389 0.1109 0.1226 0.1936 0.4039], shape=(10,), dtype=float32) \n",
      "\n",
      "\n",
      "\n",
      "~~~~~~~~~~ ~~~~~~~~~~ ~~~~~~~~~~ ~~~~~~~~~~ ~~~~~~~~~~\n",
      "Encoder Attention Normalized [0.2622 0.7205 0.1896 0.9971 0.0000 1.0000 0.0000 0.9778 0.0000 0.7124]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "~~~~~~~~~~ SEQUENCE BACKTRACE ~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Source_Code/Transformer.py:495: RuntimeWarning: invalid value encountered in true_divide\n",
      "  normalized_weights =(att-np.min(att))/(np.max(att)-np.min(att))\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../Source_Code/Spell_result/linux.log_templates.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-b18d12d342aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#fn att are for false negatives\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0matt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtp_att\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn_att\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_mat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_abnormal_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ntp-ddos\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-112ab6ee95b2>\u001b[0m in \u001b[0;36mcreate_mat\u001b[0;34m(dataloader, model, layers, heads, file, enc, dec)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"~~~~~~~~~~ SEQUENCE BACKTRACE ~~~~~~~~~~\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0mad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbacktrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"linux.log\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Spell\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"~~~~~~~~~~ LABEL BACKTRACE ~~~~~~~~~~\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/data2/tad/Dataset/ctdd-2021-os-syslogs/Source_Code/preprocess.py\u001b[0m in \u001b[0;36mbacktrace\u001b[0;34m(pred, log_source, algorithm)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbacktrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_source\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;31m#log_template = pd.read_csv(algorithm + \"_result/practicum_and_abnormal/\" + log_source + \"_templates.csv\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m     \u001b[0mlog_template\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../Source_Code/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0malgorithm\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_result/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlog_source\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_templates.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m \u001b[0;31m#     y = np.squeeze(pred.tolist())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../Source_Code/Spell_result/linux.log_templates.csv'"
     ]
    }
   ],
   "source": [
    "#print backtrack of logs\n",
    "#tp att are for true positives\n",
    "#fn att are for false negatives\n",
    "\n",
    "att, tp_att, fn_att = create_mat(test_abnormal_loader, model, layers, heads, \"ntp-ddos\", enc=True, dec=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
